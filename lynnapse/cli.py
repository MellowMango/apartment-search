"""
Lynnapse CLI interface.
"""

import asyncio
import json
import logging
from pathlib import Path
from typing import Optional

import typer
from rich.console import Console
from rich.logging import RichHandler
from rich.progress import Progress, SpinnerColumn, TextColumn

from lynnapse.config import get_settings, get_seed_loader
from lynnapse.db import get_client
from lynnapse.scrapers.university.arizona_psychology import ArizonaPsychologyScraper

# New modular components
try:
    from lynnapse.flows import main_scrape_flow
    FLOWS_AVAILABLE = True
except ImportError:
    main_scrape_flow = None
    FLOWS_AVAILABLE = False

# Adaptive scraping components
try:
    from lynnapse.core import AdaptiveFacultyCrawler
    ADAPTIVE_AVAILABLE = True
except ImportError:
    AdaptiveFacultyCrawler = None
    ADAPTIVE_AVAILABLE = False

# Legacy imports (may not exist)
try:
    from lynnapse.scrapers import ScraperOrchestrator
    from lynnapse.flows.scrape import run_university_scrape
except ImportError:
    ScraperOrchestrator = None
    run_university_scrape = None


app = typer.Typer(
    name="lynnapse",
    help="Lynnapse - University Program & Faculty Scraper",
    rich_markup_mode="rich"
)
console = Console()


def setup_logging(level: str = "INFO"):
    """Setup logging with Rich handler."""
    logging.basicConfig(
        level=getattr(logging, level.upper()),
        format="%(message)s",
        datefmt="[%X]",
        handlers=[RichHandler(rich_tracebacks=True)]
    )


@app.command()
def scrape_university(
    university: str = typer.Argument(..., help="University identifier (e.g., 'arizona-psychology')"),
    output_format: str = typer.Option("json", "--format", "-f", help="Output format: json, mongodb"),
    output_file: Optional[str] = typer.Option(None, "--output", "-o", help="Output file path"),
    include_profiles: bool = typer.Option(False, "--profiles", help="Scrape detailed faculty profiles (slower)"),
    verbose: bool = typer.Option(False, "--verbose", "-v", help="Verbose logging"),
):
    """Scrape university faculty using specialized scrapers."""
    log_level = "DEBUG" if verbose else "INFO"
    setup_logging(log_level)
    
    async def run_university_scrape():
        nonlocal output_file  # Fix scope issue
        try:
            # Select scraper based on university identifier
            if university == "arizona-psychology":
                async with ArizonaPsychologyScraper() as scraper:
                    console.print(f"üéì Scraping {scraper.university_name} - {scraper.department}")
                    
                    faculty_data = await scraper.scrape_all_faculty(
                        include_detailed_profiles=include_profiles
                    )
                    
                    # Output results
                    if output_format == "json":
                        if not output_file:
                            timestamp = int(asyncio.get_event_loop().time())
                            output_file = f"arizona_psychology_faculty_{timestamp}.json"
                        
                        with open(output_file, 'w', encoding='utf-8') as f:
                            json.dump(faculty_data, f, indent=2, ensure_ascii=False)
                        
                        console.print(f"üíæ Saved {len(faculty_data)} faculty to {output_file}")
                    
                    elif output_format == "mongodb":
                        # TODO: Implement MongoDB saving
                        console.print("üì¶ MongoDB output not yet implemented")
                    
                    # Display statistics
                    console.print(f"\nüìä Scraping Results:")
                    console.print(f"   Total Faculty: {len(faculty_data)}")
                    console.print(f"   With Email: {sum(1 for f in faculty_data if f.get('email'))}")
                    console.print(f"   With Personal Website: {sum(1 for f in faculty_data if f.get('personal_website'))}")
                    console.print(f"   With Lab: {sum(1 for f in faculty_data if f.get('lab_name'))}")
                    
                    return faculty_data
            else:
                console.print(f"[red]Unknown university identifier: {university}[/red]")
                console.print("[yellow]Available: arizona-psychology[/yellow]")
                return None
                
        except Exception as e:
            console.print(f"[red]‚úó Scrape failed: {e}[/red]")
            if verbose:
                import traceback
                console.print(traceback.format_exc())
            return None
    
    asyncio.run(run_university_scrape())


@app.command()
def test_scraper(
    university: str = typer.Argument("arizona-psychology", help="University scraper to test"),
    save_results: bool = typer.Option(True, "--save/--no-save", help="Save test results to file"),
):
    """Test university scrapers with sample data."""
    
    async def run_test():
        try:
            if university == "arizona-psychology":
                from lynnapse.scrapers.test_scrapers import test_arizona_psychology_scraper
                
                console.print("üß™ Testing Arizona Psychology scraper...")
                faculty_data = await test_arizona_psychology_scraper(
                    save_to_json=save_results,
                    include_profiles=False
                )
                
                console.print(f"‚úÖ Test completed: {len(faculty_data)} faculty scraped")
                
            else:
                console.print(f"[red]Unknown university scraper: {university}[/red]")
                console.print("[yellow]Available: arizona-psychology[/yellow]")
                
        except Exception as e:
            console.print(f"[red]‚úó Test failed: {e}[/red]")
    
    asyncio.run(run_test())


@app.command()
def scrape(
    university: str = typer.Argument(..., help="University name to scrape"),
    program: Optional[str] = typer.Option(None, "--program", "-p", help="Specific program to scrape"),
    output_dir: str = typer.Option("output", "--output", "-o", help="Output directory"),
    max_concurrent: int = typer.Option(3, "--concurrent", "-c", help="Max concurrent requests"),
    headless: bool = typer.Option(True, "--headless/--no-headless", help="Run browser in headless mode"),
    save_html: bool = typer.Option(False, "--save-html", help="Save raw HTML files"),
    verbose: bool = typer.Option(False, "--verbose", "-v", help="Verbose logging"),
):
    """Scrape a university's programs and faculty (legacy seed-based scraper)."""
    
    if run_university_scrape is None:
        console.print("[red]Legacy scraper not available. Use 'scrape-university' command instead.[/red]")
        return
    
    log_level = "DEBUG" if verbose else "INFO"
    setup_logging(log_level)
    
    logger = logging.getLogger(__name__)
    
    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        console=console,
    ) as progress:
        task = progress.add_task("Initializing scraper...", total=None)
        
        async def run_scrape():
            try:
                # Load seed configuration
                seed_loader = get_seed_loader()
                university_config = seed_loader.get_university_config(university)
                
                if not university_config:
                    console.print(f"[red]University '{university}' not found in seed configurations[/red]")
                    console.print(f"[yellow]Available universities: {list(seed_loader.universities.keys())}[/yellow]")
                    return
                
                progress.update(task, description="Starting scrape job...")
                
                # Run the scrape
                result = await run_university_scrape(
                    university_name=university,
                    program_name=program,
                    output_directory=output_dir,
                    max_concurrent=max_concurrent,
                    headless=headless,
                    save_html=save_html
                )
                
                if result["success"]:
                    console.print(f"[green]‚úì Scrape completed successfully![/green]")
                    console.print(f"[blue]Programs scraped: {result.get('programs_scraped', 0)}[/blue]")
                    console.print(f"[blue]Faculty scraped: {result.get('faculty_scraped', 0)}[/blue]")
                    console.print(f"[blue]Lab sites scraped: {result.get('lab_sites_scraped', 0)}[/blue]")
                else:
                    console.print(f"[red]‚úó Scrape failed: {result.get('error', 'Unknown error')}[/red]")
                
                progress.update(task, description="Scrape completed")
                
            except Exception as e:
                logger.error(f"Scrape failed: {e}")
                console.print(f"[red]‚úó Scrape failed: {e}[/red]")
        
        asyncio.run(run_scrape())


@app.command("adaptive-scrape")
def adaptive_scrape(
    university_name: str = typer.Argument(..., help="University name (e.g., 'Arizona State University')"),
    department: Optional[str] = typer.Option(None, "--department", "-d", help="Target specific department (e.g., 'psychology')"),
    max_faculty: Optional[int] = typer.Option(None, "--max-faculty", "-m", help="Maximum number of faculty to extract"),
    base_url: Optional[str] = typer.Option(None, "--base-url", "-u", help="Base URL if known"),
    output: Optional[str] = typer.Option(None, "--output", "-o", help="Output file path (JSON)"),
    enable_lab_discovery: bool = typer.Option(True, "--lab-discovery/--no-lab-discovery", help="Enable enhanced lab discovery features"),
    enable_external_search: bool = typer.Option(False, "--external-search/--no-external-search", help="Enable external search APIs (costs money)"),
    verbose: bool = typer.Option(False, "--verbose", "-v", help="Enable verbose logging"),
):
    """
    Scrape faculty from any university using adaptive strategies.
    
    This command can automatically discover and adapt to any university's
    website structure without manual configuration.
    
    Examples:
    
        # Scrape psychology faculty from Arizona State University
        lynnapse adaptive-scrape "Arizona State University" -d psychology -m 10
        
        # Scrape all faculty from Stanford University
        lynnapse adaptive-scrape "Stanford University" -o stanford_faculty.json
        
        # Scrape with enhanced lab discovery and external search
        lynnapse adaptive-scrape "MIT" --external-search -v
    """
    if not ADAPTIVE_AVAILABLE:
        console.print("[red]Adaptive scraping not available. Please check lynnapse.core module.[/red]")
        return
    
    # Configure logging
    log_level = "DEBUG" if verbose else "INFO"
    setup_logging(log_level)
    
    async def run_adaptive_scrape():
        console.print(f"üéØ Starting adaptive scrape for [bold]{university_name}[/bold]")
        
        if department:
            console.print(f"üìö Targeting department: [blue]{department}[/blue]")
        if max_faculty:
            console.print(f"üë• Max faculty limit: [blue]{max_faculty}[/blue]")
        
        # Initialize the adaptive crawler
        crawler = AdaptiveFacultyCrawler(
            enable_lab_discovery=enable_lab_discovery,
            enable_external_search=enable_external_search
        )
        
        try:
            with Progress(
                SpinnerColumn(),
                TextColumn("[progress.description]{task.description}"),
                console=console,
            ) as progress:
                task = progress.add_task("Discovering university structure...", total=None)
                
                result = await crawler.scrape_university_faculty(
                    university_name=university_name,
                    department_filter=department,
                    max_faculty=max_faculty,
                    base_url=base_url
                )
                
                progress.update(task, description="Scraping completed")
            
            # Display results
            if result["success"]:
                console.print(f"[green]‚úÖ Successfully scraped {result['metadata']['total_faculty']} faculty[/green]")
                console.print(f"üèõÔ∏è  Base URL: [blue]{result['base_url']}[/blue]")
                console.print(f"üìä Departments processed: [blue]{result['metadata']['departments_processed']}[/blue]")
                console.print(f"üéØ Discovery confidence: [blue]{result['metadata']['discovery_confidence']:.2f}[/blue]")
                
                # Show department breakdown
                if result['metadata']['department_results']:
                    console.print("\n[bold]üìö Department Results:[/bold]")
                    for dept_name, dept_info in result['metadata']['department_results'].items():
                        console.print(f"  ‚Ä¢ [green]{dept_name}[/green]: {dept_info['faculty_count']} faculty "
                                     f"(confidence: {dept_info['confidence']:.2f})")
                
                # Show sample faculty
                if result["faculty"]:
                    console.print(f"\n[bold]üë• Sample Faculty (showing first 3):[/bold]")
                    for i, faculty in enumerate(result["faculty"][:3]):
                        console.print(f"  {i+1}. [green]{faculty['name']}[/green]")
                        if faculty.get('title'):
                            console.print(f"     Title: [blue]{faculty['title']}[/blue]")
                        if faculty.get('email'):
                            console.print(f"     Email: [blue]{faculty['email']}[/blue]")
                        if faculty.get('lab_urls'):
                            console.print(f"     Lab URLs: [yellow]{len(faculty['lab_urls'])} found[/yellow]")
                        console.print()
                
                # Show crawler statistics
                stats = crawler.get_stats()
                if any(stats.values()):
                    console.print("[bold]üìà Crawler Statistics:[/bold]")
                    for key, value in stats.items():
                        if value > 0:
                            console.print(f"  ‚Ä¢ {key.replace('_', ' ').title()}: [blue]{value}[/blue]")
                
                # Save output if requested
                if output:
                    output_path = Path(output)
                    with output_path.open('w') as f:
                        json.dump(result, f, indent=2, default=str)
                    console.print(f"üíæ Results saved to [blue]{output_path}[/blue]")
                
            else:
                console.print(f"[red]‚ùå Scraping failed: {result['error']}[/red]")
                return 1
        
        except Exception as e:
            console.print(f"[red]üí• Unexpected error: {e}[/red]")
            if verbose:
                import traceback
                console.print(traceback.format_exc())
            return 1
        
        finally:
            await crawler.close()
        
        return 0
    
    asyncio.run(run_adaptive_scrape())


@app.command()
def list_universities():
    """List available universities in seed configurations."""
    console.print("[bold]University Scrapers:[/bold]")
    console.print("[green]‚Ä¢ arizona-psychology[/green] - University of Arizona Psychology Department")
    console.print("  Captures: Faculty profiles, personal websites, lab information")
    console.print()
    
    seed_loader = get_seed_loader()
    
    if not seed_loader.universities:
        console.print("[yellow]No seed-based universities found[/yellow]")
        console.print("[blue]Run 'lynnapse create-seed' to create seed configurations[/blue]")
        return
    
    console.print("[bold]Seed-based Universities:[/bold]")
    
    for name, config in seed_loader.universities.items():
        console.print(f"[green]‚Ä¢ {name}[/green]")
        console.print(f"  [blue]Base URL: {config.base_url}[/blue]")
        console.print(f"  [blue]Programs: {len(config.programs)}[/blue]")
        for program in config.programs:
            console.print(f"    - {program.name} ({program.department})")
        console.print()


@app.command()
def create_seed(
    university_name: str = typer.Argument(..., help="University name"),
    output_path: Optional[str] = typer.Option(None, "--output", "-o", help="Output path for seed file"),
):
    """Create a new seed configuration file."""
    if not output_path:
        safe_name = university_name.lower().replace(" ", "_").replace("-", "_")
        output_path = f"seeds/{safe_name}.yml"
    
    seed_loader = get_seed_loader()
    
    # Create a basic template
    template = {
        "name": university_name,
        "base_url": f"https://{university_name.lower().replace(' ', '')}.edu",
        "programs": [
            {
                "name": "Example Program",
                "department": "Example Department",
                "college": "Example College",
                "program_url": f"https://{university_name.lower().replace(' ', '')}.edu/program",
                "faculty_directory_url": f"https://{university_name.lower().replace(' ', '')}.edu/faculty",
                "program_type": "graduate",
                "selectors": {
                    "faculty_links": ".faculty-list a, .people-list a",
                    "faculty_name": "h1, .name",
                    "faculty_title": ".title, .position",
                    "faculty_email": "a[href^='mailto:']",
                    "research_interests": ".research-interests"
                }
            }
        ],
        "scraping_config": {
            "user_agent": "Mozilla/5.0 (compatible; LynnapseBot/1.0)",
            "wait_for_selector": ".content, .main",
            "timeout": 30
        },
        "rate_limit_delay": 2.0,
        "max_concurrent_requests": 2,
        "max_retries": 3,
        "retry_delay": 5.0
    }
    
    # Ensure directory exists
    output_path = Path(output_path)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    # Write the template
    import yaml
    with open(output_path, 'w', encoding='utf-8') as f:
        yaml.dump(template, f, default_flow_style=False, indent=2)
    
    console.print(f"[green]‚úì Created seed template: {output_path}[/green]")
    console.print("[blue]Edit the file to customize selectors and URLs for your university[/blue]")


@app.command()
def test_db():
    """Test database connection."""
    async def test_connection():
        try:
            client = await get_client()
            healthy = await client.health_check()
            
            if healthy:
                console.print("[green]‚úì Database connection successful![/green]")
                console.print(f"[blue]Database: {client.database_name}[/blue]")
            else:
                console.print("[red]‚úó Database connection failed![/red]")
                
        except Exception as e:
            console.print(f"[red]‚úó Database connection failed: {e}[/red]")
    
    asyncio.run(test_connection())


@app.command()
def init():
    """Initialize Lynnapse configuration and database."""
    console.print("[bold]Initializing Lynnapse...[/bold]")
    
    async def initialize():
        try:
            # Test database connection
            console.print("üîå Testing database connection...")
            client = await get_client()
            healthy = await client.health_check()
            
            if healthy:
                console.print("[green]‚úì Database connected[/green]")
            else:
                console.print("[red]‚úó Database connection failed[/red]")
                return
            
            console.print("[green]‚úì Lynnapse initialized successfully![/green]")
            
        except Exception as e:
            console.print(f"[red]‚úó Initialization failed: {e}[/red]")
    
    asyncio.run(initialize())


@app.command()
def flow(
    seeds_file: str = typer.Argument("seeds/university_of_arizona.yml", help="Path to seeds YAML file"),
    university: Optional[str] = typer.Option(None, "--university", "-u", help="Filter by university name"),
    department: Optional[str] = typer.Option(None, "--department", "-d", help="Filter by department name"),
    max_programs: int = typer.Option(3, "--max-programs", help="Max concurrent programs"),
    max_faculty: int = typer.Option(5, "--max-faculty", help="Max concurrent faculty"),
    max_labs: int = typer.Option(2, "--max-labs", help="Max concurrent labs"),
    dry_run: bool = typer.Option(False, "--dry-run", help="Validate configuration only"),
    verbose: bool = typer.Option(False, "--verbose", "-v", help="Verbose logging"),
):
    """Run the new modular scraping flow with Prefect orchestration."""
    
    if not FLOWS_AVAILABLE:
        console.print("[red]Modular flows not available. Please install requirements and check lynnapse.flows module.[/red]")
        return
    
    log_level = "DEBUG" if verbose else "INFO"
    setup_logging(log_level)
    
    async def run_flow():
        try:
            console.print("üéì [bold]Starting Lynnapse Modular Scraping Flow[/bold]")
            console.print(f"üìÅ Seeds file: {seeds_file}")
            if university:
                console.print(f"üè´ University filter: {university}")
            if department:
                console.print(f"üèõÔ∏è Department filter: {department}")
            
            # Run the Prefect flow
            result = await main_scrape_flow(
                seeds_file=seeds_file,
                university_filter=university,
                department_filter=department,
                max_concurrent_programs=max_programs,
                max_concurrent_faculty=max_faculty,
                max_concurrent_labs=max_labs,
                dry_run=dry_run
            )
            
            if dry_run:
                console.print("[green]‚úì Configuration validation successful![/green]")
                console.print(f"Universities: {result['configuration']['total_universities']}")
                console.print(f"Programs: {result['configuration']['total_programs']}")
            else:
                console.print("[green]‚úì Scraping flow completed successfully![/green]")
                console.print(f"Job ID: {result.get('job_id')}")
                console.print(f"Universities: {result.get('universities_processed')}")
                console.print(f"Programs: {result.get('programs_processed')}")
                console.print(f"Faculty: {result.get('faculty_processed')}")
                console.print(f"Execution time: {result.get('execution_time_seconds', 0):.1f}s")
            
            return result
            
        except Exception as e:
            console.print(f"[red]‚úó Flow failed: {e}[/red]")
            if verbose:
                import traceback
                console.print(traceback.format_exc())
            return None
    
    asyncio.run(run_flow())


@app.command()
def web(
    host: str = typer.Option("0.0.0.0", "--host", help="Host to bind to"),
    port: int = typer.Option(8000, "--port", help="Port to bind to"),
    reload: bool = typer.Option(False, "--reload", help="Enable auto-reload for development"),
):
    """Start the web interface for interactive scraping."""
    
    try:
        from lynnapse.web.app import create_app
        import uvicorn
        WEB_AVAILABLE = True
    except ImportError:
        WEB_AVAILABLE = False
    
    if not WEB_AVAILABLE:
        console.print("[red]‚ùå Web interface not available[/red]")
        console.print("[yellow]üí° Install with: pip install fastapi uvicorn jinja2 python-multipart[/yellow]")
        raise typer.Exit(1)
    
    console.print("[blue]üåê Starting Lynnapse Web Interface...[/blue]")
    console.print(f"[green]üìç Access at: http://localhost:{port}[/green]")
    console.print(f"[cyan]üìä API docs at: http://localhost:{port}/docs[/cyan]")
    console.print("[yellow]üîß Press Ctrl+C to stop[/yellow]")
    console.print()
    
    try:
        app = create_app()
        uvicorn.run(
            app,
            host=host,
            port=port,
            reload=reload,
            log_level="info"
        )
    except KeyboardInterrupt:
        console.print("\n[yellow]üëã Web interface stopped[/yellow]")
    except Exception as e:
        console.print(f"[red]‚ùå Failed to start web interface: {e}[/red]")
        raise typer.Exit(1)


@app.command()
def validate_websites(
    input_file: str = typer.Argument(..., help="Path to JSON file containing faculty data"),
    output_file: Optional[str] = typer.Option(None, "--output", "-o", help="Output file for validated data"),
    report_file: Optional[str] = typer.Option(None, "--report", "-r", help="Output file for validation report"),
    verbose: bool = typer.Option(False, "--verbose", "-v", help="Show detailed output"),
    max_concurrent: int = typer.Option(5, "--concurrent", "-c", help="Maximum concurrent requests"),
    min_confidence: float = typer.Option(0.0, "--min-confidence", help="Minimum confidence threshold for filtering")
):
    """
    Validate faculty website links from scraped data.
    
    This tool checks the accessibility and categorizes different types of links
    (personal websites, Google Scholar, university profiles, etc.).
    """
    import subprocess
    import sys
    
    # Build command arguments
    cmd = [sys.executable, "-m", "lynnapse.cli.validate_websites", input_file]
    
    if output_file:
        cmd.extend(["--output", output_file])
    if report_file:
        cmd.extend(["--report", report_file])
    if verbose:
        cmd.append("--verbose")
    if max_concurrent != 5:
        cmd.extend(["--concurrent", str(max_concurrent)])
    if min_confidence != 0.0:
        cmd.extend(["--min-confidence", str(min_confidence)])
    
    # Execute the validation script
    try:
        result = subprocess.run(cmd, check=True)
        return result.returncode
    except subprocess.CalledProcessError as e:
        console.print(f"[red]‚ùå Validation failed with exit code {e.returncode}[/red]")
        raise typer.Exit(e.returncode)
    except Exception as e:
        console.print(f"[red]‚ùå Failed to run validation: {e}[/red]")
        raise typer.Exit(1)


@app.command()
def find_better_links(
    input_file: str = typer.Argument(..., help="Path to JSON file containing faculty data"),
    output_file: Optional[str] = typer.Option(None, "--output", "-o", help="Output file for enhanced data"),
    targets_file: Optional[str] = typer.Option(None, "--targets", "-t", help="Output file for scraping targets"),
    validate_first: bool = typer.Option(True, "--validate/--no-validate", help="Run validation first"),
    dry_run: bool = typer.Option(False, "--dry-run", help="Only identify candidates, don't scrape"),
    max_concurrent: int = typer.Option(3, "--concurrent", "-c", help="Maximum concurrent requests"),
    verbose: bool = typer.Option(False, "--verbose", "-v", help="Show detailed output")
):
    """
    Find better academic links for faculty with poor quality links.
    
    This tool identifies faculty with social media links, broken links, or unknown links,
    then performs targeted searches to find their actual academic websites.
    """
    import subprocess
    import sys
    
    # Build command arguments
    cmd = [sys.executable, "lynnapse/cli/secondary_scraping.py", input_file]
    
    if output_file:
        cmd.extend(["--output", output_file])
    if targets_file:
        cmd.extend(["--targets", targets_file])
    if not validate_first:
        cmd.append("--no-validate")
    if dry_run:
        cmd.append("--dry-run")
    if max_concurrent != 3:
        cmd.extend(["--concurrent", str(max_concurrent)])
    if verbose:
        cmd.append("--verbose")
    
    # Execute the secondary scraping script
    try:
        result = subprocess.run(cmd, check=True)
        return result.returncode
    except subprocess.CalledProcessError as e:
        console.print(f"[red]‚ùå Secondary scraping failed with exit code {e.returncode}[/red]")
        raise typer.Exit(e.returncode)
    except Exception as e:
        console.print(f"[red]‚ùå Failed to run secondary scraping: {e}[/red]")
        raise typer.Exit(1)


@app.command()
def version():
    """Show version information."""
    console.print("üéì [bold]Lynnapse[/bold] v2.0.0 - Modular Architecture")
    console.print("University Program & Faculty Scraper")
    console.print("‚ú® Features: Prefect orchestration, modular components, enhanced data cleaning")
    console.print("üìã Available commands:")
    console.print("  ‚Ä¢ [blue]web[/blue] - Start web interface (recommended)")
    console.print("  ‚Ä¢ [blue]flow[/blue] - Run modular scraping flow")
    console.print("  ‚Ä¢ [blue]scrape-university[/blue] - Run specialized scrapers")
    console.print("  ‚Ä¢ [blue]scrape[/blue] - Run legacy seed-based scraper")
    console.print("  ‚Ä¢ [blue]validate-websites[/blue] - Check and categorize faculty website links")
    console.print("  ‚Ä¢ [blue]find-better-links[/blue] - Find better academic links for faculty with poor quality links")


if __name__ == "__main__":
    app() 