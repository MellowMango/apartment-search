# Scraper Architecture

This document outlines the architecture and organization of the scraper system for Acquire Apartments. The system is designed to extract property listings from various broker websites using MCP (Model Context Protocol) servers.

## Directory Structure

```
/
├── backend/
│   └── scrapers/              # All scrapers organized by category
│       ├── core/              # Shared scraper utilities
│       │   ├── mcp_client.py  # Common MCP client code
│       │   ├── data_extractors.py  # Common data extraction functions
│       │   └── storage.py     # Database storage utilities
│       ├── brokers/           # Broker-specific scrapers
│       │   └── acrmultifamily/  # One folder per broker
│       │       ├── scraper.py  # Main scraper implementation
│       │       └── README.md   # Documentation for this specific broker
│       ├── helpers/           # Helper scripts for scrapers
│       └── run_scraper.py     # Command-line interface for running scrapers
└── data/                      # All generated data (gitignored)
    ├── screenshots/           # Screenshots organized by source
    ├── html/                  # Saved HTML content
    └── extracted/             # Extracted structured data (JSON)
```

## Core Components

### 1. MCP Client (`backend/scrapers/core/mcp_client.py`)

The MCP Client provides a common interface for interacting with Model Context Protocol servers. It supports both Firecrawl MCP and MCP-Playwright implementations, offering methods for:

- Navigating to web pages
- Retrieving HTML content
- Executing JavaScript in the browser context
- Taking screenshots

### 2. Data Extractors (`backend/scrapers/core/data_extractors.py`)

Common utilities for extracting structured data from HTML content, including:

- Property listing extraction
- Regular expression-based data parsing
- Common extraction patterns for property data (address, price, units, etc.)

### 3. Storage (`backend/scrapers/core/storage.py`)

Utilities for storing extracted data in both files and databases:

- Screenshot storage
- HTML content storage
- Structured data storage (JSON)
- Database integration (Supabase and Neo4j)

## Broker-Specific Scrapers

Each broker website has its own specialized scraper in the `backend/scrapers/brokers/` directory. Each scraper:

1. Inherits functionality from the core components
2. Implements site-specific extraction logic
3. Has its own documentation (README.md)
4. Can be run independently via the command-line interface

## Current Implementations

### ACR Multifamily Scraper

- **Target Website**: https://www.acrmultifamily.com/properties
- **Implementation**: `backend/scrapers/brokers/acrmultifamily/scraper.py`
- **Documentation**: `backend/scrapers/brokers/acrmultifamily/README.md`
- **Extracted Data**: Property listings with title, location, units, year built, status, etc.

## Data Organization

All data generated by scrapers is stored in the `data/` directory, which is excluded from git tracking. The data is organized as follows:

### Screenshots

Screenshots are stored in `data/screenshots/<source>/YYYYMMDD-HHMMSS.txt` where:
- `<source>` is the name of the broker (e.g., `acrmultifamily`)
- `YYYYMMDD-HHMMSS` is the timestamp when the screenshot was taken

### HTML Content

HTML content is stored in `data/html/<source>/YYYYMMDD-HHMMSS.html` with previews in `data/html/<source>/preview-YYYYMMDD-HHMMSS.txt`.

### Extracted Data

Structured data extracted from the websites is stored in JSON format at `data/extracted/<source>/properties-YYYYMMDD-HHMMSS.json`.

## Command-Line Interface

The system provides a command-line interface for running scrapers:

```bash
python3 backend/scrapers/run_scraper.py <scraper_name>
```

Or to run all scrapers:

```bash
python3 backend/scrapers/run_scraper.py all
```

## Database Integration

Extracted data can be stored in both Supabase (PostgreSQL) and Neo4j:

1. Property listings are stored in Supabase for relational queries
2. Graph relationships (broker-property, broker-brokerage, etc.) are stored in Neo4j
3. Celery tasks synchronize data between Supabase and Neo4j

## Future Enhancements

1. **Additional Broker Scrapers**: Implement scrapers for other broker websites listed in `docs/links-to-scrape.md`
2. **Scheduled Scraping**: Set up periodic scraping jobs using Celery
3. **Email Integration**: Process property listings from broker emails
4. **OCR Processing**: Extract data from images in emails or on websites
5. **LLM-Guided Extraction**: Enhance extraction using LLMs for adaptive scraping

## Usage Example

```python
from backend.scrapers.brokers.acrmultifamily.scraper import ACRMultifamilyScraper
import asyncio

async def main():
    scraper = ACRMultifamilyScraper()
    results = await scraper.extract_properties()
    
    if results.get("success"):
        properties = results.get("properties", [])
        print(f"Extracted {len(properties)} properties")
        for prop in properties[:3]:  # Print first 3 properties
            print(f"- {prop['title']} ({prop.get('location', 'N/A')}): {prop.get('units', 'N/A')} units")
    else:
        print(f"Error: {results.get('error')}")

if __name__ == "__main__":
    asyncio.run(main())
```

## Adding a New Broker Scraper

To add a new broker scraper:

1. Create a new directory in `backend/scrapers/brokers/` for the broker
2. Implement the scraper using the core components
3. Add documentation in a README.md file
4. Register the scraper in `backend/scrapers/run_scraper.py`
5. Add tests for the scraper

See `backend/scrapers/brokers/acrmultifamily/` for a reference implementation. 